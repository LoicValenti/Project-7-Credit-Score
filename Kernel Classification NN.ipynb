{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n","metadata":{"_uuid":"a334a3525c03f23a55668f55db390abca81eab20","_kg_hide-output":true,"_cell_guid":"a5f1f752-8365-40a8-a849-8ab2ab34db63","execution":{"iopub.status.busy":"2022-12-12T14:08:05.323522Z","iopub.execute_input":"2022-12-12T14:08:05.324406Z","iopub.status.idle":"2022-12-12T14:08:05.349108Z","shell.execute_reply.started":"2022-12-12T14:08:05.324305Z","shell.execute_reply":"2022-12-12T14:08:05.348308Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/home-credit-manual-engineered-features/m_train_small.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-12-12T14:08:05.350948Z","iopub.execute_input":"2022-12-12T14:08:05.351597Z","iopub.status.idle":"2022-12-12T14:08:26.451010Z","shell.execute_reply.started":"2022-12-12T14:08:05.351561Z","shell.execute_reply":"2022-12-12T14:08:26.450020Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from sklearn.impute import SimpleImputer\ntarget = train[\"TARGET\"]\nX_train = train.drop(columns = [\"TARGET\"])\nimputer = SimpleImputer()\nX_train = imputer.fit_transform(X_train)\n","metadata":{"execution":{"iopub.status.busy":"2022-12-12T14:08:26.452691Z","iopub.execute_input":"2022-12-12T14:08:26.453065Z","iopub.status.idle":"2022-12-12T14:08:30.495940Z","shell.execute_reply.started":"2022-12-12T14:08:26.453031Z","shell.execute_reply":"2022-12-12T14:08:30.494923Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"from keras import backend as K\nimport tensorflow as tf\nimport keras\nfrom keras import layers\nfrom keras import models\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.decomposition import PCA\n\n\nfrom imblearn.over_sampling import SMOTE\n\nstd_scaler = StandardScaler()\nX_train = std_scaler.fit_transform(X_train)\npca = PCA(n_components = 25)\nsm = SMOTE(k_neighbors = 110)\nX_train, y_train = sm.fit_resample(X_train, target)\n\n\n#X_train = pca.fit_transform(X_train)\n\n#print(pca.explained_variance_ratio_.cumsum())\n\n\n\n\n\n#pca = PCA(n_components = 50)\n\n#X_train = pca.fit_transform(X_train)\n#X_train = umap.UMAP(n_components=122).fit_transform(X_train)\nX_train, X_test, y_train, y_test = train_test_split(\n    X_train,\n    y_train,\n    test_size=0.1)\n#X_train, y_train = sm.fit_resample(X_train, y_train)\n\n\n\"\"\"\nprint(\"### AutoFeat with %i feateng_steps\" % 1)\nafreg = AutoFeatClassifier(verbose=1, feateng_steps=1, max_gb = 0.50, \n                           transformations = (\"exp\", \"^2\", '^3', \"abs\"))\ndf = afreg.fit_transform(X_train, y_train)\nr2 = afreg.score(X_test, y_test)\nprint(\"## Final R^2: %.4f\" % r2)\nplt.figure()\nplt.scatter(afreg.predict(X_test), y_test, s=2);\nplt.title(\"%i FE steps (R^2: %.4f; %i new features)\" % (1, r2, len(afreg.new_feat_cols_)))\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2022-12-12T14:08:30.498357Z","iopub.execute_input":"2022-12-12T14:08:30.499042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"counts = np.bincount(y_train)\nprint(\n    \"Number of positive samples in training data: {} ({:.2f}% of total)\".format(\n        counts[1], 100 * float(counts[1]) / len(y_train)\n    )\n)\n\nweight_for_0 = 1.0 / counts[0]\nweight_for_1 = 1.0 / counts[1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras import regularizers","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = keras.Sequential(\n    [\n        keras.layers.Dense(256, activation=\"relu\", input_shape=(X_train.shape[-1],)),\n        keras.layers.Dropout(0.5),\n        keras.layers.Dense(16, activation=\"relu\"),\n        keras.layers.Dense(4, activation=\"relu\"),\n        keras.layers.Dense(16, activation=\"relu\"),\n        keras.layers.Dense(256, activation=\"relu\"),\n        keras.layers.Dropout(0.5),\n        keras.layers.Dense(1, activation=\"sigmoid\"),\n    ]\n)\nmodel.summary()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(X_train).isna().sum().value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\n\ndef f1(y_true, y_pred):\n    y_pred = K.round(y_pred)\n    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n\n    p = tp / (tp + fp + K.epsilon())\n    r = tp / (tp + fn + K.epsilon())\n\n    f1 = 2*p*r / (p+r+K.epsilon())\n    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n    return K.mean(f1)\n\ndef f1_loss(y_true, y_pred):\n    \n    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n\n    p = tp / (tp + fp + K.epsilon())\n    r = tp / (tp + fn + K.epsilon())\n\n    f1 = 2*p*r / (p+r+K.epsilon())\n    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n    return 1 - K.mean(f1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"metrics = [tf.keras.metrics.FalseNegatives(name=\"fn\"),\n    tf.keras.metrics.FalsePositives(name=\"fp\"),\n    tf.keras.metrics.TrueNegatives(name=\"tn\"),\n    tf.keras.metrics.TruePositives(name=\"tp\"),\n    tf.keras.metrics.Precision(name=\"precision\"),\n    tf.keras.metrics.Recall(name=\"recall\"),\n    tf.keras.metrics.AUC(name = \"auc\"),\n    f1\n]\n\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adamax(learning_rate=1e-3), loss=f1_loss, metrics=[metrics]\n)\n\n\n#class_weight = {0: weight_for_0, 1: weight_for_1}\n\nhistory = model.fit(\n    X_train,\n    y_train,\n    batch_size=X_train.shape[0],\n    epochs=200,\n    validation_data=(X_test, y_test),\n    #class_weight = class_weight\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nhistory_dict = history.history\nloss_values = history_dict[\"loss\"]\nval_loss_values = history_dict[\"val_loss\"]\nepochs = range(1, len(loss_values) + 1)\nplt.plot(epochs, loss_values, \"bo\", label=\"Training loss\")\nplt.plot(epochs, val_loss_values, \"b\", label=\"Validation loss\")\nplt.title(\"Training and validation loss\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.clf()\nacc = history_dict[\"fp\"]\nval_acc = history_dict[\"val_fp\"]\nplt.plot(epochs, acc, \"bo\", label=\"Training fp\")\nplt.plot(epochs, val_acc, \"b\", label=\"Validation fp\")\nplt.title(\"Training and validation fp\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"fp\")\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.clf()\nacc = history_dict[\"fn\"]\nval_acc = history_dict[\"val_fn\"]\nplt.plot(epochs, acc, \"bo\", label=\"Training fn\")\nplt.plot(epochs, val_acc, \"b\", label=\"Validation fn\")\nplt.title(\"Training and validation fn\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"fp\")\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = pd.read_csv(\"/kaggle/input/home-credit-manual-engineered-features/m_test_small.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#test = test.align(train, axis = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ntest = imputer.transform(test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#pca = PCA(n_components = 50)\n\n\ntest = std_scaler.transform(test)\n#test = pca.fit_transform(test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_model = model.predict(test)\nsubmit = pd.read_csv(\"/kaggle/input/test-imputed-credit-score/test_imputed.csv\")[['SK_ID_CURR']]\nsubmit['TARGET'] = test_model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submit.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submit.to_csv(\"test_model_NN.csv\", index = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}